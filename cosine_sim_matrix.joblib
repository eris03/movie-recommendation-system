import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from ast import literal_eval
from nltk.stem.snowball import SnowballStemmer
import joblib

print("--- Movie Recommender Model Creation ---")

# --- 1. Load All Datasets ---
print("Step 1: Loading all required datasets...")
md = pd.read_csv('../input/movies_metadata.csv', low_memory=False)
credits = pd.read_csv('../input/credits.csv')
keywords = pd.read_csv('../input/keywords.csv')
links_small = pd.read_csv('../input/links_small.csv')

# --- 2. Initial Cleaning and Filtering to Small Dataset ---
print("Step 2: Cleaning and filtering data to the small dataset...")
# Safely drop known bad rows from the original metadata file
md = md.drop([19730, 29503, 35587], errors='ignore')

# Clean and convert ID columns
md['id'] = pd.to_numeric(md['id'], errors='coerce')
md.dropna(subset=['id'], inplace=True)
md['id'] = md['id'].astype('int')

# Get the list of movie IDs from the small dataset
links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')
small_movie_ids = set(links_small)

# Filter the main dataframe to only include movies from the small dataset
smd = md[md['id'].isin(small_movie_ids)].copy()
print(f"   > Original 'small' dataset size: {len(smd)} movies.")

# --- 3. Filter to Top N Most Popular Movies ---
print("Step 3: Filtering down to the most popular movies to reduce model size...")
# Calculate the components for the Weighted Rating (IMDB's formula)
vote_counts = smd[smd['vote_count'].notnull()]['vote_count'].astype('int')
vote_averages = smd[smd['vote_average'].notnull()]['vote_average'].astype('float')
C = vote_averages.mean()
m = vote_counts.quantile(0.60) # Set the minimum votes required to be in the 60th percentile

# Define the weighted rating function
def weighted_rating(x):
    v = x['vote_count']
    R = x['vote_average']
    return (v / (v + m) * R) + (m / (m + v) * C)

# Apply the function and sort to find the most popular movies
smd['wr'] = smd.apply(weighted_rating, axis=1)
N = 2500 # Define the number of top movies to keep
smd = smd.sort_values('wr', ascending=False).head(N)
print(f"   > Filtered down to the top {N} most popular movies.")

# --- 4. Feature Engineering ('soup' creation) ---
print("Step 4: Engineering features from metadata (cast, director, genres, keywords)...")
# Merge with credits and keywords dataframes
smd = smd.merge(credits, on='id')
smd = smd.merge(keywords, on='id')

# Define a safe function for literal_eval to prevent errors on re-runs
def safe_literal_eval(val):
    if isinstance(val, list): return val
    try: return literal_eval(val)
    except: return []

for feature in ['genres', 'cast', 'crew', 'keywords']:
    smd[feature] = smd[feature].apply(safe_literal_eval)

# Extract director, top 3 cast, genres, and stem keywords
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return ''
smd['director'] = smd['crew'].apply(get_director)

for feature in ['cast', 'keywords', 'genres']:
    smd[feature] = smd[feature].apply(lambda x: [i['name'] for i in x])
smd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >= 3 else x)

stemmer = SnowballStemmer('english')
smd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])

def sanitize(x):
    if isinstance(x, list):
        return [str.lower(i.replace(" ", "")) for i in x]
    return str.lower(x.replace(" ", "")) if isinstance(x, str) else ''

for feature in ['cast', 'keywords', 'director', 'genres']:
    smd[feature] = smd[feature].apply(sanitize)

# Create the final metadata "soup"
smd['director'] = smd['director'].apply(lambda x: [x, x, x]) # Weight director
smd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']
smd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))
print("   > Feature 'soup' created.")

# --- 5. Vectorization and Similarity Calcula
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from ast import literal_eval
from nltk.stem.snowball import SnowballStemmer
import joblib

print("--- Movie Recommender Model Creation ---")

# --- 1. Load All Datasets ---
print("Step 1: Loading all required datasets...")
md = pd.read_csv('../input/movies_metadata.csv', low_memory=False)
credits = pd.read_csv('../input/credits.csv')
keywords = pd.read_csv('../input/keywords.csv')
links_small = pd.read_csv('../input/links_small.csv')

# --- 2. Initial Cleaning and Filtering to Small Dataset ---
print("Step 2: Cleaning and filtering data to the small dataset...")
# Safely drop known bad rows from the original metadata file
md = md.drop([19730, 29503, 35587], errors='ignore')

# Clean and convert ID columns
md['id'] = pd.to_numeric(md['id'], errors='coerce')
md.dropna(subset=['id'], inplace=True)
md['id'] = md['id'].astype('int')

# Get the list of movie IDs from the small dataset
links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')
small_movie_ids = set(links_small)

# Filter the main dataframe to only include movies from the small dataset
smd = md[md['id'].isin(small_movie_ids)].copy()
print(f"   > Original 'small' dataset size: {len(smd)} movies.")

# --- 3. Filter to Top N Most Popular Movies ---
print("Step 3: Filtering down to the most popular movies to reduce model size...")
# Calculate the components for the Weighted Rating (IMDB's formula)
vote_counts = smd[smd['vote_count'].notnull()]['vote_count'].astype('int')
vote_averages = smd[smd['vote_average'].notnull()]['vote_average'].astype('float')
C = vote_averages.mean()
m = vote_counts.quantile(0.60) # Set the minimum votes required to be in the 60th percentile

# Define the weighted rating function
def weighted_rating(x):
    v = x['vote_count']
    R = x['vote_average']
    return (v / (v + m) * R) + (m / (m + v) * C)

# Apply the function and sort to find the most popular movies
smd['wr'] = smd.apply(weighted_rating, axis=1)
N = 2500 # Define the number of top movies to keep
smd = smd.sort_values('wr', ascending=False).head(N)
print(f"   > Filtered down to the top {N} most popular movies.")
# --- 4. Feature Engineering ('soup' creation) ---
print("Step 4: Engineering features from metadata (cast, director, genres, keywords)...")
# Merge with credits and keywords dataframes
smd = smd.merge(credits, on='id')
smd = smd.merge(keywords, on='id')

# Define a safe function for literal_eval to prevent errors on re-runs
def safe_literal_eval(val):
    if isinstance(val, list): return val
    try: return literal_eval(val)
    except: return []

for feature in ['genres', 'cast', 'crew', 'keywords']:
    smd[feature] = smd[feature].apply(safe_literal_eval)

# Extract director, top 3 cast, genres, and stem keywords
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return ''
smd['director'] = smd['crew'].apply(get_director)

for feature in ['cast', 'keywords', 'genres']:
    smd[feature] = smd[feature].apply(lambda x: [i['name'] for i in x])
smd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >= 3 else x)

stemmer = SnowballStemmer('english')
smd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])

def sanitize(x):
    if isinstance(x, list):
        return [str.lower(i.replace(" ", "")) for i in x]
    return str.lower(x.replace(" ", "")) if isinstance(x, str) else ''

for feature in ['cast', 'keywords', 'director', 'genres']:
    smd[feature] = smd[feature].apply(sanitize)

# Create the final metadata "soup"
smd['director'] = smd['director'].apply(lambda x: [x, x, x]) # Weight director
smd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']
smd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))
print("   > Feature 'soup' created.")

# --- 5. Vectorization and Similarity Calculation ---
print("Step 5: Calculating the final similarity matrix...")
count = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')
count_matrix = count.fit_transform(smd['soup'])
cosine_sim = cosine_similarity(count_matrix, count_matrix)
cosine_sim = cosine_sim.astype(np.float16) # Convert to float16 to save space
print(f"   > Final matrix of shape {cosine_sim.shape} created.")
print(f"   > Final matrix size: {cosine_sim.nbytes / 1024**2:.2f} MB")

# --- 6. Save the Final Model Components ---
print("Step 6: Saving the final model components to disk...")
# We only need the movie titles and the similarity matrix for the app
smd = smd.reset_index()
final_df = smd[['title', 'id']].copy() 

joblib.dump(final_df, 'movies_df.joblib')
joblib.dump(cosine_sim, 'cosine_sim_matrix.joblib')

print("\nâœ… Success! The following files have been created:")
print("   1. movies_df.joblib")
print("   2. cosine_sim_matrix.joblib")
